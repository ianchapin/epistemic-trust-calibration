# epistemic-trust-calibration
A lightweight framework for calibrating trust in representations (models, metrics, explanations) under uncertainty. Focused on explicit scope, ambiguity typing, confidence gating, and judgment handoff. Not a truth system or authority.
